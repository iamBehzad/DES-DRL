{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamBehzad/DES-DRL/blob/main/DES_DRL_14020430.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5FCAcM15sW8",
        "outputId": "041005b6-5eee-4501-9d50-d12ca0c3bb7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: DESlib in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from DESlib) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from DESlib) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from DESlib) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.0->DESlib) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.0->DESlib) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install DESlib;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "YFGnrwq0anDz"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "from deslib.dcs import LCA\n",
        "from deslib.dcs import MLA\n",
        "from deslib.dcs import OLA\n",
        "from deslib.dcs import MCB\n",
        "from deslib.dcs import Rank\n",
        "from deslib.des import KNORAE, KNORAU, KNOP, METADES, DESKNN, DESClustering\n",
        "from deslib.static.oracle import Oracle\n",
        "from deslib.static.single_best import SingleBest\n",
        "from deslib.util.datasets import make_P2\n",
        "\n",
        "import sklearn.preprocessing as preprocessing\n",
        "import scipy.io as sio\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "import math\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k6a4A_2uIhp"
      },
      "source": [
        "#DES-DRL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "uycFwCEJuMNR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "from scipy.stats import mode\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnpERIYixHkP"
      },
      "source": [
        "#Enviorment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "6KLVH6whmdTy"
      },
      "outputs": [],
      "source": [
        "class EnsembleSelectionEnv:\n",
        "    def __init__(self, X_train, y_train, X_test, pool_classifiers, n_estimators):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_test = X_test\n",
        "\n",
        "        self.pool_classifiers=pool_classifiers\n",
        "\n",
        "        self.max_steps = 20\n",
        "        self.min_steps = 1\n",
        "        self.min_accuracy = 0.8\n",
        "\n",
        "        self.num_classes = len(np.unique(self.y_train))\n",
        "        self.n_estimators = n_estimators\n",
        "        self.acc_threshold = 0.6\n",
        "        self.current_ensemble = []\n",
        "        self.current_step = 0\n",
        "        self.current_index = -1\n",
        "        self.current_state = None\n",
        "        self.done = False\n",
        "        self.current_ensemble_num_member = 0\n",
        "\n",
        "        self.current_sample_X = self.X_test[self.current_index]\n",
        "        self.current_sample_X_num_feature = self.current_sample_X.shape[0]\n",
        "\n",
        "        self.k=7\n",
        "        self.knn_classifier = KNeighborsClassifier(n_neighbors=self.k)\n",
        "        self.knn_classifier.fit(self.X_train, self.y_train)\n",
        "        knn_distance, self.knn_indices = self.knn_classifier.kneighbors(self.current_sample_X.reshape(1, -1))\n",
        "        self.knn = np.array([self.X_train[i] for i in self.knn_indices])[0]\n",
        "        self.knn_class = np.array([self.y_train[i] for i in self.knn_indices])[0]\n",
        "\n",
        "        self.classifiers_neighbors_hard_classification = np.zeros((self.n_estimators,self.k))\n",
        "        self.classifiers_Overall_accuracy=np.zeros(len(self.pool_classifiers))\n",
        "        self.classifiers_predictions = np.zeros((self.n_estimators,self.k))\n",
        "\n",
        "        x_2d = np.reshape(self.knn, (-1, self.current_sample_X_num_feature))\n",
        "        predictions = np.vstack([c.predict(x_2d) for c in self.pool_classifiers])\n",
        "        self.classifiers_predictions[:,:] = predictions\n",
        "        self.classifiers_neighbors_hard_classification [:,:] = self.classifiers_predictions == self.knn_class\n",
        "        correct = np.sum(self.classifiers_predictions == self.knn_class, axis=1)\n",
        "        self.classifiers_Overall_accuracy = correct / len(self.classifiers_predictions)\n",
        "\n",
        "        num_ones = np.random.randint(5, 10)\n",
        "        ones_indices = random.sample(range(self.n_estimators), num_ones)\n",
        "        self.current_ensemble = np.array([1 if i in ones_indices else 0 for i in range(self.n_estimators)])\n",
        "\n",
        "        self.current_classifiers = [self.pool_classifiers[i] for i in range(len(self.current_ensemble)) if self.current_ensemble[i] == 1]\n",
        "\n",
        "        self.ensemble_predictions = np.zeros(self.k)\n",
        "        self.ensemble_neighbors_hard_classification = np.zeros((self.k))\n",
        "\n",
        "        self.ensemble_Overall_accuracy = 0\n",
        "\n",
        "        self.observation_space = {\n",
        "            'current_ensemble': (self.current_ensemble,),\n",
        "            'current_ensemble_num_member': 1,\n",
        "            'ensemble_predictions': self.ensemble_predictions,\n",
        "            'ensemble_neighbors_hard_classification': self.ensemble_neighbors_hard_classification,\n",
        "            'ensemble_Overall_accuracy': 1,\n",
        "            'classifiers_predictions': self.classifiers_predictions.flatten(),\n",
        "            'classifiers_neighbors_hard_classification': self.classifiers_neighbors_hard_classification.flatten(),\n",
        "            'classifiers_Overall_accuracy': self.classifiers_Overall_accuracy,\n",
        "            'k_nearest_neighbors_class' : self.knn_class,\n",
        "        }\n",
        "\n",
        "        self.action_space_n = 3\n",
        "        self.observation_space = len(*self.observation_space[\"current_ensemble\"]) + \\\n",
        "                                     self.observation_space[\"current_ensemble_num_member\"] + \\\n",
        "                                     len(self.observation_space[\"ensemble_predictions\"]) + \\\n",
        "                                     len(self.observation_space[\"ensemble_neighbors_hard_classification\"]) + \\\n",
        "                                     self.observation_space[\"ensemble_Overall_accuracy\"] + \\\n",
        "                                     len(self.observation_space[\"classifiers_predictions\"]) + \\\n",
        "                                     len(self.observation_space[\"classifiers_neighbors_hard_classification\"]) + \\\n",
        "                                     len(self.observation_space[\"classifiers_Overall_accuracy\"]) + \\\n",
        "                                     len(self.observation_space[\"k_nearest_neighbors_class\"])\n",
        "        #print(\"self.observation_space: \" , self.observation_space)\n",
        "\n",
        "    def _update_ensemble_stats(self):\n",
        "      current_ensemble_num_estimator = np.count_nonzero(self.current_ensemble == 1)\n",
        "      self.current_ensemble_num_member = current_ensemble_num_estimator / self.n_estimators # Normalized No. Members of Current Ensemble\n",
        "\n",
        "      for i in range(self.k):\n",
        "          curr_ensemble = self.current_ensemble.astype(bool)\n",
        "          curr_preds = self.classifiers_predictions[:,i][curr_ensemble]\n",
        "          mode_pred =int(mode(curr_preds, axis=0)[0][0])\n",
        "          self.ensemble_predictions[i] = self.current_classifiers[0].classes_[mode_pred]\n",
        "          if self.ensemble_predictions[i] == self.knn_class[i]:\n",
        "              self.ensemble_neighbors_hard_classification[i] = 1\n",
        "      self.ensemble_Overall_accuracy = np.sum(self.ensemble_predictions == self.knn_class) / self.k\n",
        "\n",
        "    def _add_member(self):\n",
        "      inverted_current_ensemble = [not m for m in self.current_ensemble]\n",
        "      high_acc_cls = [x > self.acc_threshold for x in self.classifiers_Overall_accuracy]\n",
        "      eligible_clf = [a and b for a, b in zip(inverted_current_ensemble, high_acc_cls)]\n",
        "      if all(x == 0 for x in eligible_clf):\n",
        "          alternative_action = np.random.randint(0, env.action_space_n)\n",
        "          self.step(alternative_action)\n",
        "      else:\n",
        "          chosen_list = [i for i, x in enumerate(eligible_clf) if x != 0]\n",
        "          best_idx = random.choice(chosen_list)\n",
        "          new_member = self.pool_classifiers[best_idx]\n",
        "          self.current_ensemble[best_idx] = 1\n",
        "          self.current_classifiers.append(new_member)\n",
        "          self._update_ensemble_stats()\n",
        "\n",
        "    def _replace_member(self):\n",
        "      inverted_current_ensemble = [not m for m in self.current_ensemble]\n",
        "      high_acc_cls = [x > self.acc_threshold for x in self.classifiers_Overall_accuracy]\n",
        "      eligible_clf = [a and b for a, b in zip(inverted_current_ensemble, high_acc_cls)]\n",
        "      if all(x == 0 for x in eligible_clf):\n",
        "          alternative_action = np.random.randint(0, env.action_space_n)\n",
        "          self.step(alternative_action)\n",
        "      elif not all(x == 0 for x in self.current_ensemble):\n",
        "          chosen_list = [i for i, x in enumerate(eligible_clf) if x != 0]\n",
        "          best_idx = random.choice(chosen_list)\n",
        "\n",
        "          replace_idx = np.random.choice(np.where(self.current_ensemble == 1)[0])\n",
        "          self.current_ensemble[replace_idx] = 0\n",
        "          self.current_ensemble[best_idx] = 1\n",
        "          self.current_classifiers.clear()\n",
        "          self.current_classifiers = [self.pool_classifiers[i] for i in range(len(self.current_ensemble)) if self.current_ensemble[i] == 1]\n",
        "          self._update_ensemble_stats()\n",
        "\n",
        "    def _remove_member(self):\n",
        "        chosen_list = [i for i, x in enumerate(self.current_ensemble) if x != 0]\n",
        "        if len(chosen_list) > 1 :\n",
        "          remove_idx = random.choice(chosen_list)\n",
        "          self.current_ensemble[remove_idx] = 0\n",
        "          self.current_classifiers.clear()\n",
        "          self.current_classifiers = [self.pool_classifiers[i] for i in range(len(self.current_ensemble)) if self.current_ensemble[i] == 1]\n",
        "          self._update_ensemble_stats()\n",
        "        else :\n",
        "          alternative_action = np.random.randint(0, env.action_space_n)\n",
        "          self.step(alternative_action)\n",
        "\n",
        "    def step(self, action):\n",
        "      self.current_step += 1\n",
        "      if self.current_step < self.max_steps:\n",
        "        if action == 0:\n",
        "            self._add_member()\n",
        "        elif action == 1:\n",
        "            self._replace_member()\n",
        "        elif action == 2:\n",
        "            self._remove_member()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid action: {action}\")\n",
        "\n",
        "      next_state = self._get_obs()\n",
        "      self.current_state = next_state\n",
        "\n",
        "      reward =  self.ensemble_Overall_accuracy\n",
        "\n",
        "      if self.current_step >= self.max_steps:\n",
        "          self.done = True\n",
        "      if self.current_step >= self.min_steps and self.ensemble_Overall_accuracy > self.min_accuracy:\n",
        "          self.done = True\n",
        "      if self.done:\n",
        "          return next_state, reward, self.done,{tuple((tuple(self.current_sample_X), tuple(self.current_ensemble), self.current_ensemble_num_member)) }\n",
        "\n",
        "      return next_state, reward, self.done, {()}\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.done = False\n",
        "        self.current_index +=1\n",
        "        self.current_sample_X = self.X_test[self.current_index]\n",
        "        knn_distance, self.knn_indices = self.knn_classifier.kneighbors(self.current_sample_X.reshape(1, -1))\n",
        "        self.knn = np.array([self.X_train[i] for i in self.knn_indices])[0]\n",
        "        self.knn_class = np.array([self.y_train[i] for i in self.knn_indices])[0]\n",
        "\n",
        "        x_2d = np.reshape(self.knn, (-1, self.current_sample_X_num_feature))\n",
        "        predictions = np.vstack([c.predict(x_2d) for c in self.pool_classifiers])\n",
        "        self.classifiers_predictions[:,:] = predictions\n",
        "        self.classifiers_neighbors_hard_classification [:,:] = self.classifiers_predictions == self.knn_class\n",
        "        correct = np.sum(self.classifiers_predictions == self.knn_class, axis=1)\n",
        "        self.classifiers_Overall_accuracy = correct / len(self.classifiers_predictions)\n",
        "\n",
        "        num_ones = np.random.randint(5, 10)\n",
        "        ones_indices = random.sample(range(self.n_estimators), num_ones)\n",
        "        self.current_ensemble =np.array( [1 if i in ones_indices else 0 for i in range(self.n_estimators)])\n",
        "        self.current_state = self._get_obs()\n",
        "        return self.current_state\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = {\n",
        "                'current_ensemble': [*self.current_ensemble] ,\n",
        "                'current_ensemble_num_member': self.current_ensemble_num_member,\n",
        "                'ensemble_predictions': self.ensemble_predictions,\n",
        "                'ensemble_neighbors_hard_classification': self.ensemble_neighbors_hard_classification,\n",
        "                'ensemble_Overall_accuracy': self.ensemble_Overall_accuracy,\n",
        "                'classifiers_predictions': np.concatenate(self.classifiers_predictions),\n",
        "                'classifiers_neighbors_hard_classification': np.concatenate(self.classifiers_neighbors_hard_classification) ,\n",
        "                'classifiers_Overall_accuracy': self.classifiers_Overall_accuracy ,\n",
        "                'k_nearest_neighbors_class' : self.knn_class,\n",
        "              }\n",
        "        return *obs['current_ensemble'], obs['current_ensemble_num_member'], *obs['ensemble_predictions'],\\\n",
        "               *obs['ensemble_neighbors_hard_classification'],obs['ensemble_Overall_accuracy'],\\\n",
        "               *obs['classifiers_predictions'],\\\n",
        "               *obs['classifiers_neighbors_hard_classification'],*obs['classifiers_Overall_accuracy'],\\\n",
        "               *obs[\"k_nearest_neighbors_class\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7GpBtssu4HB"
      },
      "source": [
        "#Parameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "WKWeD6qZuxBh"
      },
      "outputs": [],
      "source": [
        "# if gpu is to be used\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "Tensor = torch.Tensor\n",
        "LongTensor = torch.LongTensor\n",
        "seed_value = 23\n",
        "torch.manual_seed(seed_value)\n",
        "random.seed(seed_value)\n",
        "###### PARAMS ######\n",
        "learning_rate = 0.001\n",
        "gamma = 0.9999\n",
        "hidden_layer = 64\n",
        "replay_mem_size = 50000\n",
        "batch_size = 32\n",
        "update_target_frequency = 500\n",
        "double_dqn = True\n",
        "report_interval = 10\n",
        "score_to_solve = 195\n",
        "clip_error = False\n",
        "egreedy = 0.9\n",
        "egreedy_final = 0.01\n",
        "egreedy_decay = 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSDD268BvCFJ"
      },
      "source": [
        "#Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "m1t_V06wuw5K"
      },
      "outputs": [],
      "source": [
        "class ExperienceReplay(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "    def push(self, state, action, new_state, reward, done):\n",
        "        transition = (state, action, new_state, reward, done)\n",
        "        if self.position >= len(self.memory):\n",
        "            self.memory.append(transition)\n",
        "        else:\n",
        "            self.memory[self.position] = transition\n",
        "        self.position = ( self.position + 1 ) % self.capacity\n",
        "    def sample(self, batch_size):\n",
        "        return zip(*random.sample(self.memory, batch_size))\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer)\n",
        "        self.linear2 = nn.Linear(hidden_layer,number_of_outputs)\n",
        "        #self.activation = nn.Tanh()\n",
        "        self.activation = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        output1 = self.linear1(x)\n",
        "        output1 = self.activation(output1)\n",
        "        output2 = self.linear2(output1)\n",
        "        return output2\n",
        "\n",
        "class QNet_Agent(object):\n",
        "    def __init__(self):\n",
        "        self.nn = NeuralNetwork().to(device)\n",
        "        self.target_nn = NeuralNetwork().to(device)\n",
        "        self.loss_func = nn.MSELoss()\n",
        "        #self.loss_func = nn.SmoothL1Loss()\n",
        "        #self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
        "        self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)\n",
        "        self.update_target_counter = 0\n",
        "    def select_action(self,state,epsilon):\n",
        "        random_for_egreedy = torch.rand(1)[0]\n",
        "        if random_for_egreedy > epsilon:\n",
        "            with torch.no_grad():\n",
        "                state = Tensor(state).to(device)\n",
        "                action_from_nn = self.nn(state)\n",
        "                action = torch.max(action_from_nn,0)[1]\n",
        "                action = action.item()\n",
        "        else:\n",
        "            action = np.random.randint(0, env.action_space_n)\n",
        "        return action\n",
        "    def optimize(self):\n",
        "        if (len(memory) < batch_size):\n",
        "            return\n",
        "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
        "        state = Tensor(state).to(device)\n",
        "        new_state = Tensor(new_state).to(device)\n",
        "        reward = Tensor(reward).to(device)\n",
        "        action = LongTensor(action).to(device)\n",
        "        done = Tensor(done).to(device)\n",
        "        if double_dqn:\n",
        "            new_state_indexes = self.nn(new_state).detach()\n",
        "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1]\n",
        "            new_state_values = self.target_nn(new_state).detach()\n",
        "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
        "        else:\n",
        "            new_state_values = self.target_nn(new_state).detach()\n",
        "            max_new_state_values = torch.max(new_state_values, 1)[0]\n",
        "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
        "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "        loss = self.loss_func(predicted_value, target_value)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if clip_error:\n",
        "            for param in self.nn.parameters():\n",
        "                param.grad.data.clamp_(-1,1)\n",
        "        self.optimizer.step()\n",
        "        if self.update_target_counter % update_target_frequency == 0:\n",
        "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
        "        self.update_target_counter += 1\n",
        "        #Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
        "\n",
        "def calculate_epsilon(steps_done):\n",
        "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
        "              math.exp(-1. * steps_done / egreedy_decay )\n",
        "    return epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49vviP20vJVh"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "L73XZoaQvIgR"
      },
      "outputs": [],
      "source": [
        "def Train(num_episodes):\n",
        "    for i_episode in range(num_episodes):\n",
        "\n",
        "      state = env.reset()\n",
        "      step = 0\n",
        "      frames_total = 0\n",
        "      done = False\n",
        "      rewards=[]\n",
        "      while not done:\n",
        "          step += 1\n",
        "          frames_total += 1\n",
        "          epsilon = calculate_epsilon(frames_total)\n",
        "          #print(\"epsilon :\", epsilon)\n",
        "          action = qnet_agent.select_action(state, epsilon)\n",
        "          new_state, reward, done, info = env.step(action)\n",
        "          memory.push(state, action, new_state, reward, done)\n",
        "          qnet_agent.optimize()\n",
        "          state = new_state\n",
        "          rewards.append(reward)\n",
        "          if done:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8eEVJaHvIYS"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "NtZU8XQevLFq"
      },
      "outputs": [],
      "source": [
        "def Test(num_episodes):\n",
        "    selected_ensemble = []\n",
        "    num_member = []\n",
        "    classifiers_predictions = []\n",
        "    overall_accuracy_selected_ensemble = []\n",
        "    accuracy_selected_ensemble = []\n",
        "    diversity_selected_ensemble = []\n",
        "    num_episodes = len(X_test)\n",
        "    X_test_data = []\n",
        "    y_hat = []\n",
        "    total_reward = []\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        state = test_env.reset()\n",
        "        #step = 0\n",
        "        frames_total = 0\n",
        "        done = False\n",
        "        rewards=0\n",
        "        while not done:\n",
        "            #step += 1\n",
        "            frames_total += 1\n",
        "            #epsilon = calculate_epsilon(frames_total)\n",
        "            epsilon = 0\n",
        "            action = qnet_agent.select_action(state, epsilon)\n",
        "            new_state, reward, done, info = test_env.step(action)\n",
        "            memory.push(state, action, new_state, reward, done)\n",
        "            #qnet_agent.optimize()\n",
        "            state = new_state\n",
        "            rewards+=reward\n",
        "            if done:\n",
        "              info_list = list(info)\n",
        "              x=info_list[0][0]\n",
        "              X_test_data.append(info_list[0][0])\n",
        "              EoC=info_list[0][1]\n",
        "              selected_ensemble.append(EoC)\n",
        "              num_member.append(info_list[0][2])\n",
        "              current_classifiers = [pool_classifiers[i] for i in range(len(EoC)) if EoC[i] == 1]\n",
        "              predictions = [c.predict(np.array(x).reshape(1, -1))[0] for c in current_classifiers]\n",
        "              classifiers_predictions.append(predictions)\n",
        "              ensemble_predictions = mode(predictions, axis=0, keepdims=True)\n",
        "              predicted_class_index = ensemble_predictions.mode[0]\n",
        "              y_hat.append(current_classifiers[0].classes_[predicted_class_index])\n",
        "              total_reward.append(rewards)\n",
        "              break\n",
        "    return y_hat, selected_ensemble, num_member"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiEq0gNOuxQZ"
      },
      "source": [
        "#Test Platform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDiGFSio5KX5",
        "outputId": "afdb05fa-8e61-4f2a-c3d6-d40a6d3b0ace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heart\n",
            "Heart is readed.\n",
            "Heart :  (270, 13)\n",
            "Train time 93.47653675079346\n",
            "Test time 252.46945905685425\n"
          ]
        }
      ],
      "source": [
        "# Prepare the DS techniques. Changing k value to 7.\n",
        "\n",
        "def initialize_ds(pool_classifiers, X_DSEL, y_DSEL, k=7):\n",
        "    knorau = KNORAU(pool_classifiers, k=k)\n",
        "    kne = KNORAE(pool_classifiers, k=k)\n",
        "    desknn = DESKNN(pool_classifiers, k=k)\n",
        "    ola = OLA(pool_classifiers, k=k)\n",
        "    lca = LCA(pool_classifiers, k=k)\n",
        "    mla = MLA(pool_classifiers, k=k)\n",
        "    mcb = MCB(pool_classifiers, k=k)\n",
        "    rank = Rank(pool_classifiers, k=k)\n",
        "    knop = KNOP(pool_classifiers, k=k)\n",
        "    meta = METADES(pool_classifiers, k=k)\n",
        "\n",
        "    single_best = SingleBest(pool_classifiers, n_jobs=-1)\n",
        "    oracle = Oracle(pool_classifiers)\n",
        "    majority_voting = pool_classifiers\n",
        "\n",
        "    list_ds = [knorau, kne, desknn, ola, lca, mla, mcb, rank, knop, meta, single_best, oracle]\n",
        "    methods_names = ['KNORA-U', 'KNORAE', 'DESKNN', 'OLA', 'LCA', 'MLA', 'MCB', 'Rank', 'KNOP', 'META-DES', 'SingleBest', 'Oracle']\n",
        "\n",
        "    # fit the ds techniques\n",
        "    for ds in list_ds:\n",
        "        if ds != majority_voting:\n",
        "            ds.fit(X_DSEL, y_DSEL)\n",
        "\n",
        "\n",
        "    return list_ds, methods_names\n",
        "\n",
        "def save_pool(datasetName,pools):\n",
        "    path = ExperimentPath + \"/Pools/\" + datasetName + \"_pools.p\"\n",
        "    poolspec = open(path, mode=\"wb\")\n",
        "    pickle.dump(pools, poolspec)\n",
        "    poolspec.close()\n",
        "def load_pool(datasetName):\n",
        "    path = ExperimentPath + \"/Pools/\" + datasetName + \"_pools.p\"\n",
        "    poolspec = open(path, mode=\"rb\")\n",
        "    return pickle.load(poolspec)\n",
        "\n",
        "def save_model(tec_name,datasetName,ds):\n",
        "    path = ExperimentPath + \"/Models/\" + tec_name +\"_\"+datasetName + \"_model.p\"\n",
        "    poolspec = open(path, mode=\"wb\")\n",
        "    pickle.dump(ds, poolspec)\n",
        "    poolspec.flush()\n",
        "    poolspec.close()\n",
        "def load_model(tec_name,datasetName):\n",
        "    path = ExperimentPath + \"/Models/\" + tec_name +\"_\"+ datasetName + \"_model.p\"\n",
        "    poolspec = open(path, mode=\"rb\")\n",
        "    return pickle.load(poolspec)\n",
        "\n",
        "def save_results(tec_name,datasetName,accuracy,labels,yhat):\n",
        "    path = ExperimentPath + \"/Results/\" + tec_name +\"_\"+datasetName + \"_result.p\"\n",
        "    poolspec = open(path, mode=\"wb\")\n",
        "    pickle.dump(accuracy, poolspec)\n",
        "    pickle.dump(labels, poolspec)\n",
        "    pickle.dump(yhat, poolspec)\n",
        "    poolspec.flush()\n",
        "    poolspec.close()\n",
        "\n",
        "def pool_generator(datasetName):\n",
        "    state = 0\n",
        "    pools = []\n",
        "    for itr in range(0, no_itr):\n",
        "        rng = np.random.RandomState(state)\n",
        "        [X_train, X_test, X_DSEL, y_train, y_test, y_DSEL] =  np.load('/content/drive/MyDrive/Experiment1/Datasets/' + datasetName +str(itr)+'.npy',allow_pickle=True)\n",
        "\n",
        "        learner = Perceptron(max_iter=100, tol=10e-3, alpha=0.001, penalty=None, random_state=rng)\n",
        "        model = CalibratedClassifierCV(learner, cv=5,method='isotonic')\n",
        "        pool_classifiers = BaggingClassifier(model, n_estimators=NO_classifiers, bootstrap=True, max_samples=1.0, random_state=rng)\n",
        "        pool_classifiers.fit(X_train,y_train)\n",
        "        pools.append(pool_classifiers)\n",
        "\n",
        "    path = ExperimentPath + \"/Pools/\" + datasetName + \"_pools.p\"\n",
        "    poolspec = open(path, mode=\"wb\")\n",
        "    pickle.dump(pools, poolspec)\n",
        "\n",
        "def model_setup(datasetName):\n",
        "    global methods_names\n",
        "    pools = load_pool(datasetName)\n",
        "    ds_matrix = []\n",
        "    for itr in range(no_itr):\n",
        "        pool_classifiers = pools[itr]\n",
        "        [X_train, X_test, X_DSEL, y_train, y_test, y_DSEL] = np.load('/content/drive/MyDrive/Experiment1/Datasets/' + datasetName + str(itr) + '.npy', allow_pickle=True)\n",
        "        list_ds, methods_names = initialize_ds(pool_classifiers,X_DSEL,y_DSEL)\n",
        "        ds_matrix.append(list_ds)\n",
        "\n",
        "    for tec in range(NO_techniques):\n",
        "        ds_tec = []\n",
        "        for itr in range(no_itr):\n",
        "            ds_tec.append(ds_matrix[itr][tec])\n",
        "        save_model(methods_names[tec],datasetName,ds_tec)\n",
        "\n",
        "def evaluate_model(datasetName):\n",
        "    for tec in range(NO_techniques):\n",
        "        results = []\n",
        "        labels = []\n",
        "        yhat = []\n",
        "        ds_tec = load_model(methods_names[tec],datasetName)\n",
        "        for itr in range(no_itr):\n",
        "            [X_train, X_test, X_DSEL, y_train, y_test, y_DSEL] = np.load('/content/drive/MyDrive/Experiment1/Datasets/' + datasetName + str(itr) + '.npy',  allow_pickle=True)\n",
        "            labels.append(y_test)\n",
        "            results.append(ds_tec[itr].score(X_test, y_test) * 100)\n",
        "            if methods_names[tec] == 'Oracle':\n",
        "                yhat.append(ds_tec[itr].predict(X_test,y_test))\n",
        "            else:\n",
        "                yhat.append(ds_tec[itr].predict(X_test))\n",
        "\n",
        "        save_results(methods_names[tec],datasetName,results,labels,yhat)\n",
        "\n",
        "def convert_datasets(datasetName):\n",
        "    redata = sio.loadmat(\"/content/drive/MyDrive/DES_DataSets2/\" + datasetName + \".mat\")\n",
        "    data = redata['dataset']\n",
        "    X = data[:, 0:-1]\n",
        "    y = data[:, -1]\n",
        "    print(datasetName, \"is readed.\")\n",
        "    state = 0\n",
        "    print(datasetName, ': ', X.shape)\n",
        "\n",
        "\n",
        "    # ### ### ### ### ### ### ### ### ###\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    y = le.fit_transform(y)\n",
        "    X[np.isnan(X)] = 0\n",
        "    # #### **** #### **** #### **** #### **** #### **** #### ****\n",
        "    # scaler = preprocessing.MinMaxScaler()\n",
        "    # X = scaler.fit_transform(X)\n",
        "    # #### **** #### **** #### **** #### **** #### **** #### ****\n",
        "\n",
        "\n",
        "    yhat = np.zeros((no_itr, math.ceil(len(y) / 4)))\n",
        "    for itr in range(0, no_itr):\n",
        "        # rand = np.random.randint(1,10000,1)\n",
        "        rng = np.random.RandomState(itr)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=y,\n",
        "                                                            random_state=rng)  # stratify=y\n",
        "        X_DSEL, X_test, y_DSEL, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test,\n",
        "                                                          random_state=rng)  # stratify=y_test\n",
        "        yhat[itr, :] = y_test\n",
        "\n",
        "\n",
        "        scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
        "        X_train = scaler.transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "        X_DSEL = scaler.transform(X_DSEL)\n",
        "\n",
        "        np.save('/content/drive/MyDrive/Experiment1/Datasets/'+ datasetName+ str(itr) ,[X_train,X_test,X_DSEL,y_train,y_test,y_DSEL])\n",
        "\n",
        "datasets = {\n",
        "    #     Data set of DGA1033 report\n",
        "#    \"Adult\",\n",
        "#    \"Banana\",\n",
        "    \"Heart\",\n",
        "#    \"ILPD\",\n",
        "#   \"Vehicle\",\n",
        "#    \"Glass\",\n",
        "#    \"Pima\",\n",
        "#    \"Sonar\",\n",
        "#    \"Ecoli\"\n",
        "#    \"Wine\"\n",
        "#    \"Audit\",\n",
        "#    \"Banknote\",\n",
        "#    \"Blood\",\n",
        "#    \"Breast\",\n",
        "#    \"Car\",\n",
        "#    \"Datausermodeling\",\n",
        "#    \"Faults\",\n",
        "#    \"German\",\n",
        "#    \"Haberman\",\n",
        "#    \"Ionosphere\",\n",
        "#    \"Laryngeal1\",\n",
        "#    \"Laryngeal3\",\n",
        "#    \"Lithuanian\",\n",
        "#    \"Liver\",\n",
        "#    \"Mammographic\",\n",
        "#    \"Monk2\",\n",
        "#    \"Phoneme\",\n",
        "#    \"Pima\",\n",
        "#    \"Sonar\",\n",
        "#    \"Statlog\",\n",
        "#    \"Steel\",\n",
        "#    \"Thyroid\",\n",
        "#    \"Vertebral\",\n",
        "#    \"Voice3\",\n",
        "#    \"Weaning\",\n",
        "}\n",
        "\n",
        "datasets = sorted(datasets)\n",
        "\n",
        "ExperimentPath = \"/content/drive/MyDrive/Experiment1\"\n",
        "NO_classifiers =100\n",
        "no_itr = 20\n",
        "generate_pools = True\n",
        "do_train = True\n",
        "do_evaluate = True\n",
        "\n",
        "methods_names = ['KNORA-U', 'KNORAE', 'DESKNN', 'OLA', 'LCA', 'MLA', 'MCB', 'Rank', 'KNOP', 'META-DES', 'SingleBest', 'Oracle']\n",
        "NO_techniques = len(methods_names)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for datasetName in datasets:\n",
        "    # try:\n",
        "    print(datasetName)\n",
        "    convert_datasets(datasetName)\n",
        "    if generate_pools:\n",
        "        pool_generator(datasetName)\n",
        "    if do_train:\n",
        "        t1 = time.time()\n",
        "        model_setup(datasetName)\n",
        "        print(\"Train time\",time.time()-t1)\n",
        "    if do_evaluate:\n",
        "        t1 = time.time()\n",
        "        evaluate_model(datasetName)\n",
        "        print(\"Test time\", time.time() - t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0iAG-Q9vbKR"
      },
      "source": [
        "#Run DES-DRL Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNzixMe2vgEZ",
        "outputId": "27e3c178-6b45-475b-9e60-cf124faeec15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration:  0 th...\n",
            "Train time 17.683279752731323\n",
            "Test time 20.610486268997192\n",
            "Accuracy  0 is :  82.35294117647058\n",
            "iteration:  1 th...\n",
            "Train time 19.846223831176758\n",
            "Test time 17.57728934288025\n",
            "Accuracy  1 is :  82.35294117647058\n",
            "iteration:  2 th...\n",
            "Train time 21.105157136917114\n",
            "Test time 17.445918798446655\n",
            "Accuracy  2 is :  79.41176470588235\n",
            "iteration:  3 th...\n",
            "Train time 18.577295064926147\n",
            "Test time 19.25709080696106\n",
            "Accuracy  3 is :  82.35294117647058\n",
            "iteration:  4 th...\n",
            "Train time 19.550015687942505\n",
            "Test time 17.401657104492188\n",
            "Accuracy  4 is :  86.76470588235294\n",
            "iteration:  5 th...\n",
            "Train time 21.027182579040527\n",
            "Test time 17.602648973464966\n",
            "Accuracy  5 is :  86.76470588235294\n",
            "iteration:  6 th...\n",
            "Train time 17.793966054916382\n",
            "Test time 17.71565270423889\n",
            "Accuracy  6 is :  73.52941176470588\n",
            "iteration:  7 th...\n",
            "Train time 18.595805883407593\n",
            "Test time 17.24932622909546\n",
            "Accuracy  7 is :  86.76470588235294\n",
            "iteration:  8 th...\n",
            "Train time 18.476834297180176\n",
            "Test time 18.19307041168213\n",
            "Accuracy  8 is :  83.82352941176471\n",
            "iteration:  9 th...\n",
            "Train time 18.681174516677856\n",
            "Test time 17.411256313323975\n",
            "Accuracy  9 is :  88.23529411764706\n",
            "iteration:  10 th...\n",
            "Train time 22.787360191345215\n",
            "Test time 17.462913274765015\n",
            "Accuracy  10 is :  85.29411764705883\n",
            "iteration:  11 th...\n",
            "Train time 17.9841411113739\n",
            "Test time 17.501282930374146\n",
            "Accuracy  11 is :  83.82352941176471\n",
            "iteration:  12 th...\n",
            "Train time 18.813739776611328\n",
            "Test time 17.293983697891235\n",
            "Accuracy  12 is :  86.76470588235294\n",
            "iteration:  13 th...\n",
            "Train time 19.199960231781006\n",
            "Test time 21.266563892364502\n",
            "Accuracy  13 is :  83.82352941176471\n",
            "iteration:  14 th...\n",
            "Train time 18.366363286972046\n",
            "Test time 22.405363082885742\n",
            "Accuracy  14 is :  82.35294117647058\n",
            "iteration:  15 th...\n",
            "Train time 17.84353280067444\n",
            "Test time 18.219796895980835\n",
            "Accuracy  15 is :  82.35294117647058\n",
            "iteration:  16 th...\n",
            "Train time 20.223186016082764\n",
            "Test time 18.229011297225952\n",
            "Accuracy  16 is :  76.47058823529412\n",
            "iteration:  17 th...\n",
            "Train time 17.344996213912964\n",
            "Test time 17.5046329498291\n",
            "Accuracy  17 is :  72.05882352941177\n",
            "iteration:  18 th...\n",
            "Train time 20.339863777160645\n",
            "Test time 19.25569224357605\n",
            "Accuracy  18 is :  82.35294117647058\n",
            "iteration:  19 th...\n",
            "Train time 18.653461456298828\n",
            "Test time 20.0826473236084\n",
            "Accuracy  19 is :  83.82352941176471\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "for datasetName in datasets:\n",
        "  pools = load_pool(datasetName)\n",
        "  results = []\n",
        "  labels = []\n",
        "  yhat = []\n",
        "  for itr in range(no_itr):\n",
        "    pool_classifiers = pools[itr]\n",
        "    [X_train, X_test, X_DSEL, y_train, y_test, y_DSEL] = np.load('/content/drive/MyDrive/Experiment1/Datasets/' + datasetName + str(itr) + '.npy', allow_pickle=True)\n",
        "    env = EnsembleSelectionEnv(X_DSEL, y_DSEL, X_DSEL, pool_classifiers, NO_classifiers) # Train\n",
        "    test_env=EnsembleSelectionEnv(X_DSEL, y_DSEL, X_test, pool_classifiers,NO_classifiers) # Test\n",
        "    number_of_inputs = env.observation_space\n",
        "    number_of_outputs = env.action_space_n\n",
        "    memory = ExperienceReplay(replay_mem_size)\n",
        "    qnet_agent = QNet_Agent()\n",
        "    print('iteration: ' , itr, 'th...')\n",
        "    num_episodes = len(X_DSEL)-1\n",
        "    t1 = time.time()\n",
        "    Train(num_episodes)\n",
        "    print(\"Train time\",time.time()-t1)\n",
        "    t1 = time.time()\n",
        "    num_episodes = len(X_test)-1\n",
        "    y_hat, selected_ensemble, num_member = Test(num_episodes)\n",
        "    print(\"Test time\", time.time() - t1)\n",
        "    labels.append(y_test)\n",
        "    print(\"Accuracy \", itr, \"is : \", accuracy_score( y_test, y_hat) * 100)\n",
        "    results.append(accuracy_score( y_test, y_hat) * 100)\n",
        "    yhat.append(y_hat)\n",
        "  save_results(\"DES_DRL\",datasetName,results,labels,yhat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "Mv-oQF7mZnon"
      },
      "outputs": [],
      "source": [
        "methods_names = ['DES_DRL','KNORA-U', 'KNORAE', 'DESKNN', 'OLA', 'LCA', 'MLA', 'MCB', 'Rank', 'KNOP', 'META-DES', 'SingleBest', 'Oracle']\n",
        "for d in datasets:\n",
        "  accuracy_dict = {}\n",
        "  for m in methods_names:\n",
        "      accuracy_list = []\n",
        "      path = ExperimentPath + \"/Results/\" + m + \"_\" + d + \"_result.p\"\n",
        "      with open(path, mode=\"rb\") as f:\n",
        "          accuracy = pickle.load(f)\n",
        "      accuracy_list.append(accuracy)\n",
        "      accuracy_dict[m] = np.array(accuracy_list).reshape(-1,)\n",
        "  df = pd.DataFrame(accuracy_dict)\n",
        "  # Calculate mean, std, and rank of mean for each column\n",
        "  mean = df.mean()\n",
        "  std = df.std()\n",
        "  rank = mean.rank(ascending=False)\n",
        "  df.loc['mean'] = mean\n",
        "  df.loc['std'] = std\n",
        "  df.loc['rank'] = rank\n",
        "  save_path='/content/drive/MyDrive/Experiment1/Results_xls/'\n",
        "  writer = pd.ExcelWriter(save_path + d +\"_accuracy_results.xlsx\")\n",
        "  df.to_excel(writer)\n",
        "  writer.save()\n",
        "  #print(df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dKOCwqnZN8rpcyK47HDlEACZQbFQfnQu",
      "authorship_tag": "ABX9TyN1YBFh1xsxwKNX6uswb0/I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
